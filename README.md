# Netflix-Azure-Data-Engineering-Project
Introduction:
In this project, I built a highly scalable data pipeline for processing and analyzing Netflix dataset using Azure Data Engineering Stack. The goal was to build an end-to-end ETL solution that efficiently ingests, transforms, and visualizes data in Databricks, Delta Live Tables (DLT), Azure Synapse, and Power BI.

ðŸ”§ Tech Stack & Tools Used:
âœ… Databricks: Used for data processing, transformation, and orchestration
âœ… Delta Live Tables (DLT): Implemented incremental data processing with autoloader
âœ… Azure Data Factory (ADF): For data ingestion & workflow automation
âœ… Azure Data Lake Gen2: Storage layer for Bronze, Silver, and Gold tables
âœ… Azure Synapse Analytics: Warehouse for querying structured data
âœ… Power BI: For interactive dashboards and visualizations
âœ… GitHub: Version control & collaboration
âœ… Azure Key Vault: Secure storage of credentials
âœ… dbutils: Databricks Utilities for handling widgets, secrets, and storage

ðŸ“‚ Data Pipeline Architecture:
ðŸ“Œ Ingestion Layer: Data is loaded incrementally using Databricks Autoloader from Azure Data Lake.
ðŸ“Œ Bronze Layer (Raw Data Store): Stores raw ingested data in Delta format.
ðŸ“Œ Silver Layer (Transformations & Cleansing): Applied validations, deduplication, and aggregations.
ðŸ“Œ Gold Layer (Star Schema): Data is structured for analytics & reporting.
ðŸ“Œ Orchestration: Used Databricks Workflows and Azure Data Factory for scheduling jobs.
ðŸ“Œ Visualization: Power BI connected to Azure Synapse for interactive reporting.

ðŸ”¹ âš¡ Key Implementations & Optimization Techniques
ðŸ”¹ Incremental Loading:
âœ… Used Databricks Autoloader for continuous ingestion with checkpointing.
âœ… Ensured idempotency to avoid duplicate data loads.

ðŸ”¹ Delta Live Tables for Streaming & Batch Processing
âœ… Implemented Change Data Capture (CDC) using Delta format.
âœ… Used @dlt.table and @dlt.expect_all_or_drop() to enforce data quality checks.

ðŸ”¹ Orchestration with Conditional Execution
âœ… Implemented ForEach & If-Else Conditions to execute specific jobs based on day of execution.
âœ… Leveraged dbutils.jobs.taskValues.set() for cross-task communication.

ðŸ”¹ Optimizations using Databricks SQL
âœ… Used OPTIMIZE & ZORDER BY to speed up queries & partition pruning.
âœ… Data Skipping to reduce scan times on large datasets.

ðŸ”¹ Security & Access Control
âœ… Configured Azure Key Vault to store secrets & credentials securely.
âœ… Used dbutils.secrets.get() to retrieve authentication tokens securely.

ðŸ”¹ Outcome & Impact
ðŸ”¥ End-to-end ETL Pipeline successfully processes Netflix dataset with high efficiency & scalability.
ðŸ”¥ Automated workflow execution with Databricks Workflows & ADF reduces manual effort.
ðŸ”¥ Optimized queries & data warehouse design lead to faster analytics in Power BI.
ðŸ”¥ Fully secure environment with Azure Key Vault integration.
